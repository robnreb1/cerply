import type { FastifyInstance } from 'fastify';
import { randomUUID } from 'crypto';
import { query } from '../db';

// Preview route now attempts an LLM plan when OPENAI_API_KEY is set (dynamic import to avoid hard SDK dependency).
// Fallback is a deterministic 3–4 module outline so the UI always has something to render.

/**
 * Helper: split long text into ~800-char chunks at sentence boundaries.
 */
function splitIntoChunks(text: string, maxLen = 800) {
  const chunks: { content: string; start: number; end: number }[] = [];
  let i = 0;
  while (i < text.length) {
    const slice = text.slice(i, i + maxLen);
    // try to cut at a sentence end near the end of the slice
    const lastPunct = Math.max(
      slice.lastIndexOf('. '),
      slice.lastIndexOf('! '),
      slice.lastIndexOf('? ')
    );
    const cut = lastPunct > maxLen * 0.6 ? lastPunct + 1 : slice.length;
    const content = slice.slice(0, cut).trim();
    const start = i;
    const end = i + cut;
    if (content) {
      chunks.push({ content, start, end });
    }
    i = end;
  }
  return chunks;
}

type ModuleOutline = { id: string; title: string; summary?: string; estMinutes?: number };

function makeId(prefix: string, i: number) {
  return `${prefix}-${String(i + 1).padStart(2, "0")}`;
}

function buildHeuristicModules(topicRaw: string, max = 4): ModuleOutline[] {
  const topic = String(topicRaw || 'Your topic').trim().replace(/\s+/g, ' ');
  const parts = topic.split(/[:–—-]/).map((s) => s.trim()).filter(Boolean);

  const seeds =
    parts.length > 1
      ? parts.slice(0, max)
      : [
          `${topic}: Foundations`,
          `${topic}: Core Concepts`,
          `${topic}: Applications`,
          `${topic}: Review & Practice`,
        ];

  return seeds.slice(0, max).map((t, i) => ({
    id: makeId('mod', i),
    title: t.slice(0, 96),
    summary: i === 0 ? 'Auto-suggested outline for your first session.' : undefined,
    estMinutes: 6,
  }));
}

function outlineFromText(text: string): ModuleOutline[] {
  const t = (text || '').trim();
  if (!t) return [{ id: 'mod-01', title: 'Overview', summary: 'High-level orientation.', estMinutes: 4 }];
  return buildHeuristicModules(t, 4);
}

/**
 * This file ONLY registers routes. No app.listen, no plugin re-registers.
 * NOTE: @fastify/multipart must be registered once in src/index.ts (already done).
 */
export default async function registerIngest(app: FastifyInstance) {
  /**
   * POST /ingest/preview
   * Body: { type: "text"|"url"|"file", payload: string }
   * - "text": uses local heuristic to propose an outline
   * - "url": calls AI /extract_text then proposes outline (does NOT persist)
   * - "file": for now, uses filename placeholder to size outline
   */
  app.post('/ingest/preview', async (req, reply) => {
    try {
      const body = req.body as any;

      // Accept both old and new shapes
      // New shape: { text?, artefact?, prefs?{ level, focus, priors, timeTodayMins } }
      // Old shape: { type: 'text'|'url'|'file', payload: string }
      const type = String(body?.type || '').toLowerCase();
      const payload = typeof body?.payload === 'string' ? body.payload : undefined;

      const input = {
        text:
          typeof body?.text === 'string'
            ? body.text.trim()
            : type === 'text'
            ? (payload || '').trim()
            : undefined,
        artefact:
          body?.artefact && typeof body.artefact === 'object'
            ? body.artefact
            : type === 'url'
            ? { kind: 'url', url: payload }
            : type === 'file'
            ? { kind: 'file', text: payload }
            : undefined,
        level: body?.prefs?.level || body?.level || 'beginner',
        focus: body?.prefs?.focus || body?.focus || '',
        priors: body?.prefs?.priors || body?.priors || '',
        timeTodayMins: body?.prefs?.timeTodayMins || body?.timeTodayMins || 30,
      } as {
        text?: string;
        artefact?: { kind?: string; text?: string; url?: string };
        level?: string;
        focus?: string;
        priors?: string;
        timeTodayMins?: number;
      };

      // Derive a human topic string
      let topic = input.text ?? input.artefact?.text ?? input.artefact?.url ?? '';
      topic = (topic || '').trim();
      if (!topic && type && payload) topic = payload.trim();
      if (!topic) topic = 'a topic';

      // ---------- Attempt smart LLM planner ----------
      const SYSTEM = `
You are Cerply, an expert learning designer and subject specialist.
Your job is to:
1) Ask 2–4 concise clarifying questions when the user's request is ambiguous.
2) Propose a logical set of 3–8 modules sized to the user's stated time today (intro session); assume multi-session thereafter.
3) Prefer plain, non-fluffy titles; each module ~4–12 minutes.
4) Keep a record of any assumptions you make.

Return a strict JSON object:
{
  "questions": string[],
  "modules": [{ "id": string, "title": string, "estMinutes": number }],
  "notes": string
}
`.trim();

      const USER = [
        `Topic: ${topic}`,
        `Level: ${input.level}`,
        input.focus ? `Focus: ${input.focus}` : ``,
        input.priors ? `Prior knowledge: ${input.priors}` : ``,
        `Time available today (intro): ${input.timeTodayMins} minutes`,
        `Please: (a) ask any necessary clarifying questions; (b) propose modules sized for today; (c) add assumptions.`,
      ]
        .filter(Boolean)
        .join('\n');

      let planned: any = null;
      let llmReason: string | undefined;

      try {
        // Prefer an internal runner if available (keeps typecheck decoupled)
        let runner: any = null;
        try {
          runner = await import('../11m/run');
        } catch {}
        if (runner?.callJSON) {
          const out = await runner.callJSON({
            system: SYSTEM,
            user: USER,
            model: process.env.CERPLY_SMART_MODEL || 'gpt-4o-mini',
          });
          if (out.ok) planned = out.json;
          else llmReason = out.reason || 'llm-error';
        } else {
          // Dynamic OpenAI client (only if SDK + key present)
          let OpenAI: any = null;
          try {
            OpenAI = (await import('openai')).default;
          } catch {}
          if (OpenAI && process.env.OPENAI_API_KEY) {
            const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
            const model = process.env.CERPLY_SMART_MODEL || 'gpt-4o-mini';
            const resp: any = await client.responses.create({
              model,
              input: [
                { role: 'system', content: SYSTEM },
                { role: 'user', content: USER },
              ],
              response_format: { type: 'json_object' },
            });
            const text =
              resp?.output_text ??
              resp?.choices?.[0]?.message?.content ??
              resp?.choices?.[0]?.message?.parsed ??
              '{}';
            planned = typeof text === 'string' ? JSON.parse(text) : text;
          } else {
            llmReason = 'planner-not-available';
          }
        }
      } catch (e: any) {
        llmReason = e?.message || String(e);
      }

      if (planned?.modules?.length) {
        const modules = (planned.modules as any[]).slice(0, 8).map((m, i) => ({
          id: String(m?.id || `mod-${String(i + 1).padStart(2, '0')}`),
          title: String(m?.title || '').slice(0, 96),
          estMinutes: Math.max(4, Math.min(12, Number(m?.estMinutes) || 6)),
        }));

        reply
          .header('cache-control', 'no-store')
          .header('content-type', 'application/json; charset=utf-8')
          .header('x-api', 'ingest-preview');

        return reply.send({
          ok: true,
          modules,
          diagnostics: {
            questions: Array.isArray(planned?.questions) ? planned.questions : [],
            notes: planned?.notes ?? '',
            model: process.env.CERPLY_SMART_MODEL || 'gpt-4o-mini',
            source: 'llm',
          },
        });
      }

      // ---------- Heuristic fallback (guarantees 3–4 modules) ----------
      const fallbackModules = buildHeuristicModules(topic, 4);

      reply
        .header('cache-control', 'no-store')
        .header('content-type', 'application/json; charset=utf-8')
        .header('x-api', 'ingest-preview');

      return reply.send({
        ok: true,
        modules: fallbackModules,
        diagnostics: {
          fallback: true,
          reason: llmReason || 'no-llm',
          source: 'heuristic',
        },
      });
    } catch (err) {
      (req as any).log?.error?.({ err }, 'ingest_preview_failed');
      return reply.code(500).send({ error: 'preview_failed' });
    }
  });

  /**
   * POST /ingest/url
   * Body: { url: string }
   * Calls AI /extract_text to fetch+extract, then stores chunks in artefact_chunks.
   */
  app.post('/ingest/url', async (req, reply) => {
    try {
      const body = req.body as any;
      const url: string | undefined = body?.url;
      if (!url) {
        return reply.code(400).send({ error: 'missing_url' });
      }

      const artefactId = randomUUID();

      // Try to create a minimal artefact row (ignore if table/cols mismatch).
      try {
        await query(
          `INSERT INTO artefacts (id, type, source_uri)
           VALUES ($1, 'url', $2)`,
          [artefactId, url]
        );
      } catch (e) {
        req.log.warn({ e }, 'artefacts_insert_skipped');
      }

      const aiUrl = process.env.AI_URL || 'http://ai:8090';
      const aiRes = await fetch(`${aiUrl}/extract_text`, {
        method: 'POST',
        headers: { 'content-type': 'application/json' },
        body: JSON.stringify({ url }),
      });

      if (!aiRes.ok) {
        const t = await aiRes.text();
        req.log.error({ t }, 'ai_extract_text_failed');
        return reply.code(502).send({ error: 'ai_error' });
      }

      const data = (await aiRes.json()) as { text?: string; html?: string };
      const text = (data.text ?? '').toString().trim();
      if (!text) {
        return reply.code(422).send({ error: 'no_text_extracted' });
      }

      const parts = splitIntoChunks(text, 900);
      let idx = 0;
      for (const p of parts) {
        await query(
          `INSERT INTO artefact_chunks
           (id, artefact_id, idx, content, char_start, char_end)
           VALUES (gen_random_uuid()::text, $1, $2, $3, $4, $5)`,
          [artefactId, idx++, p.content, p.start, p.end]
        );
      }

      return reply.send({ artefactId, chunks: parts.length });
    } catch (err) {
      req.log.error({ err }, 'ingest_url_failed');
      return reply.code(500).send({ error: 'ingest_failed' });
    }
  });

  /**
   * POST /ingest/upload (txt only for now)
   * Multipart form-data with "file".
   * Reads small text file and stores chunks.
   */
  app.post('/ingest/upload', async (req, reply) => {
    try {
      // @fastify/multipart is registered in index.ts
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      const mp: any = await (req as any).file();
      if (!mp) return reply.code(400).send({ error: 'no_file' });

      const filename = mp.filename as string;
      const mimetype = mp.mimetype as string;
      const buf: Buffer =
        typeof mp.toBuffer === 'function'
          ? await mp.toBuffer()
          : await new Promise<Buffer>((resolve, reject) => {
              const chunks: Buffer[] = [];
              mp.file.on('data', (c: Buffer) => chunks.push(c));
              mp.file.on('end', () => resolve(Buffer.concat(chunks)));
              mp.file.on('error', reject);
            });

      // Only support text/plain for now to keep things simple/stable
      if (mimetype !== 'text/plain') {
        return reply.code(415).send({ error: 'unsupported_type', mimetype, filename });
      }

      const text = buf.toString('utf8').trim();
      if (!text) return reply.code(422).send({ error: 'empty_text' });

      const artefactId = randomUUID();

      try {
        await query(
          `INSERT INTO artefacts (id, type, source_uri, title)
           VALUES ($1, 'upload', $2, $3)`,
          [artefactId, filename, filename]
        );
      } catch (e) {
        req.log.warn({ e }, 'artefacts_insert_skipped');
      }

      const parts = splitIntoChunks(text, 900);
      let idx = 0;
      for (const p of parts) {
        await query(
          `INSERT INTO artefact_chunks
           (id, artefact_id, idx, content, char_start, char_end)
           VALUES (gen_random_uuid()::text, $1, $2, $3, $4, $5)`,
          [artefactId, idx++, p.content, p.start, p.end]
        );
      }

      return reply.send({ artefactId, chunks: parts.length });
    } catch (err) {
      req.log.error({ err }, 'upload_failed');
      return reply.code(500).send({ error: 'upload_failed' });
    }
  });

  /**
   * POST /curator/auto-generate
   * Body: { artefactId: string }
   * Calls AI /generate_items then persists objectives & items.
   */
  app.post('/curator/auto-generate', async (req, reply) => {
    try {
      const body = req.body as any;
      const artefactId: string | undefined = body?.artefactId;
      if (!artefactId) {
        return reply.code(400).send({ error: 'missing_artefactId' });
      }

      const { rows: chunks } = await query<{ idx: number; content: string }>(
        'SELECT idx, content FROM artefact_chunks WHERE artefact_id = $1 ORDER BY idx ASC LIMIT 20',
        [artefactId]
      );

      if (chunks.length === 0) {
        return reply.code(400).send({ error: 'no_chunks' });
      }

      const aiUrl = process.env.AI_URL || 'http://ai:8090';
      const aiRes = await fetch(`${aiUrl}/generate_items`, {
        method: 'POST',
        headers: { 'content-type': 'application/json' },
        body: JSON.stringify({
          chunks: chunks.map((c) => c.content),
          count_objectives: 3,
          items_per_objective: 4,
        }),
      });

      if (!aiRes.ok) {
        const t = await aiRes.text();
        (req as any).log?.error?.({ t }, 'ai_generate_items_failed');
        return reply.code(502).send({ error: 'ai_error' });
      }

      const gen = (await aiRes.json()) as {
        objectives: { text: string; taxonomy?: string[] }[];
        items: {
          objectiveIndex?: number;
          stem: string;
          options: string[];
          correctIndex?: number;
          correctIndices?: number[];
          explainer?: string;
          sourceSnippetRef?: string;
          difficulty?: number;
          variantGroupId?: string;
          trustMappingRefs?: string[];
        }[];
      };

      // Insert objectives
      const objectiveIds: string[] = [];
      for (const obj of gen.objectives) {
        const r = await query<{ id: string }>(
          `INSERT INTO objectives (id, artefact_id, text, taxonomy)
           VALUES (gen_random_uuid()::text, $1, $2, $3)
           RETURNING id`,
          [artefactId, obj.text, obj.taxonomy ?? []]
        );
        objectiveIds.push(r.rows[0].id);
      }

      // Insert items
      let itemsCreated = 0;
      for (const item of gen.items) {
        const oi = Math.max(0, Math.min(objectiveIds.length - 1, item.objectiveIndex ?? 0));
        const objectiveId = objectiveIds[oi];

        await query(
          `INSERT INTO items
            (id, objective_id, stem, options, correct_index, correct_indices, explainer,
             source_snippet_ref, difficulty, variant_group_id, status, version, trust_label, trust_mapping_refs)
           VALUES
            (gen_random_uuid()::text, $1, $2, $3, $4, $5, $6,
             $7, $8, $9, 'DRAFT', 1, NULL, $10)`,
          [
            objectiveId,
            item.stem,
            item.options,
            item.correctIndex ?? null,
            item.correctIndices ?? [],
            item.explainer ?? '',
            item.sourceSnippetRef ?? null,
            item.difficulty ?? null,
            item.variantGroupId ?? null,
            item.trustMappingRefs ?? [],
          ]
        );
        itemsCreated++;
      }

      return reply.send({
        ok: true,
        objectivesCreated: objectiveIds.length,
        itemsCreated,
      });
    } catch (err) {
      (req as any).log?.error?.({ err }, 'auto_generate_failed');
      return reply.code(500).send({ error: 'auto_generate_failed' });
    }
  });

  /**
   * POST /ingest/generate
   * Body: { modules: ModuleOutline[] }
   * - Produces draft content for each module (stubbed, no persistence)
   */
  app.post('/ingest/generate', async (req, reply) => {
    try {
      const body = req.body as any;
      const modules: ModuleOutline[] = Array.isArray(body?.modules) ? body.modules : [];
      if (!modules.length) {
        return reply.code(400).send({ error: 'bad_request', message: 'modules[] required' });
      }

      const items = modules.map((m: ModuleOutline, i: number) => ({
        moduleId: m.id || makeId('mod', i),
        title: m.title,
        explanation: `Overview of "${m.title}". Key takeaways are summarized in plain language to build intuition before testing.`,
        questions: {
          mcq: {
            id: `auto-${i}-${Math.random().toString(36).slice(2, 8)}`,
            stem: `Which statement best captures "${m.title}"?`,
            options: ['About: which', 'Unrelated: statement', 'Partially true: best', 'Opposite: captures'],
            correctIndex: 0,
          },
          free: {
            prompt: `In 2–3 sentences, explain "${m.title}" to a colleague.`,
          },
        },
      }));

      reply
        .header('cache-control', 'no-store')
        .header('content-type', 'application/json; charset=utf-8')
        .header('x-api', 'ingest-generate');
      return reply.send({ ok: true, items });
    } catch (err) {
      (req as any).log?.error?.({ err }, 'ingest_generate_failed');
      return reply.code(500).send({ error: 'generate_failed' });
    }
  });
}
# LLM-Driven Conversation Testing Guide

## üéØ **What We Changed**

**Before:** Hardcoded templates that always said the same thing
**After:** Every response is generated by GPT-4o following tone guidelines

---

## üß™ **Test Cases**

### Test 1: Topic Request (Anchor Point)
**Input:** `how to manage conflict in teams`

**Expected:**
- Natural confirmation of what they want to learn
- **Oxford professor tone** (understated, professional)
- Asks "Is that what you're looking for?"
- NO exclamation marks or "valuable skill" phrases

**Response "yes":**
- Should say something like "Understood. I'm structuring your learning path..."
- NOT make a new API call or interpret "yes" as a new request

---

### Test 2: Subject Request (Too Broad)
**Input:** `leadership`

**Expected:**
- Acknowledges it's broad
- Suggests narrowing to a specific topic
- Asks what specific aspect they want to focus on

---

### Test 3: Module Request (Too Specific)
**Input:** `how to write a STAR format answer`

**Expected:**
- Acknowledges the specific skill
- Confirms understanding
- Explains it will build the broader topic around it

---

### Test 4: Refinement Flow
**Input:** `particle physics`
**Response:** _(Cerply asks for clarification)_
**Refinement:** `specifically quantum mechanics`

**Expected:**
- Second response should skip praise ("I understand" or "I see")
- Should NOT repeat "excellent choice" or similar
- Should just confirm and ask if correct

---

### Test 5: Context Maintenance (CRITICAL)
**Input:** `financial planning for retirement`
**Response:** _(Cerply confirms understanding)_
**Follow-up:** `yes`

**Expected:**
- Should recognize "yes" as confirmation, NOT a new request
- Console should show: `[Cerply] Starting content generation for: financial planning for retirement`
- Should NOT make another /api/content/understand call

---

## üìä **Success Criteria**

### ‚úÖ **Quality**
- [ ] Every response is unique (try same input 3x, get 3 different responses)
- [ ] Tone is professional, understated ("Oxford professor")
- [ ] NO exclamation marks or enthusiastic language
- [ ] NO templated phrases ("valuable skill", "great choice")

### ‚ö° **Performance**
- [ ] Response time: 1-3 seconds per message
- [ ] NO timeouts (30s limit)
- [ ] NO 503 errors from OpenAI

### üß† **Context**
- [ ] Confirmation flow works ("yes" triggers generation, not new request)
- [ ] Console shows correct state detection
- [ ] NO duplicate API calls

### üí∞ **Cost**
- [ ] Each message costs ~$0.002 (acceptable)
- [ ] Conversation (5 messages) costs ~$0.01 (acceptable)

---

## üîç **How to Test**

1. **Open:** http://localhost:3000
2. **Open Console:** F12 ‚Üí Console tab
3. **Type each test case** and observe:
   - Response quality (natural? professional?)
   - Response time (1-3 seconds?)
   - Console logs (correct state detection?)
4. **Watch for context issues:**
   - After confirmation, type "yes" and check console
   - Should see `[Cerply] Starting content generation for: <original request>`
   - Should NOT see another `/api/conversation` call

---

## üö® **Red Flags (Rollback Triggers)**

### Immediate Rollback:
- Responses take >5 seconds consistently
- Frequent 503 errors (OpenAI down)
- LLM ignores tone guidelines (enthusiastic, templated)

### Monitor Closely:
- Response time 3-5 seconds (slower than expected)
- Occasional inconsistency in tone
- Cost higher than expected

### Acceptable:
- Response time 1-3 seconds
- Cost ~$0.002/message (~$10/day for 1000 users)
- Tone occasionally imperfect but mostly good

---

## üìà **Monitoring**

Check API logs for:
```bash
# In api terminal, look for:
POST /api/conversation 200 in 1500ms  # ‚úÖ Good
POST /api/conversation 200 in 5000ms  # ‚ö†Ô∏è Slow
POST /api/conversation 503 in 100ms   # üö® LLM down
```

---

## üîÑ **Rollback Command**

If it doesn't work:
```bash
cd /Users/robertford/Desktop/cerply-cursor-starter-v2-refresh
git revert 9f70688 --no-edit
```

This restores the old templated system immediately.

---

## üí° **Improvements for V2 (If Keeping)**

1. **Cache common responses** (e.g., "leadership" always gets similar first response)
2. **Use gpt-4o-mini** for confirmations (faster, cheaper)
3. **Stream responses** for perceived speed
4. **Fallback to templates** if LLM fails
5. **Add retry logic** for transient errors

---

**Status:** Ready to test
**URL:** http://localhost:3000
**Console:** Open F12 before testing
**Rollback:** `git revert 9f70688 --no-edit`

